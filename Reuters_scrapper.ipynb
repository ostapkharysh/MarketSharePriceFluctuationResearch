{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.reuters.com/resources/archive/us/{}{}{}.html\"\n",
    "ag = \"Reuters\"\n",
    "#reuters_db = '/home/ostapkharysh/Documents/bt_data/DB/Reuters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import datetime\n",
    "#from db_manager import *\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from manager import *\n",
    "from threading import Lock\n",
    "mutex = Lock()\n",
    "\n",
    "years = ['2013']#, '2014', '2015', '2016', '2017']\n",
    "months = ['01',]# '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "days = ['01', ]#'02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'\n",
    "       #'20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "    \n",
    "exec(open(\"db_management/DB.py\").read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The new table Reuters is created.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_agency(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(link):\n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        #print(r.raise_for_status())\n",
    "        soup = BS(r.content, 'html.parser')\n",
    "        head = soup.html.title.string.strip().split(\" | \")[0]\n",
    "        text = '\\n'.join([e.get_text() for e in soup.find_all('p')])\n",
    "        #print(text)\n",
    "        #print(head)\n",
    "        return head, text\n",
    "    \n",
    "    except HTTPError:\n",
    "        print(\"No article found by this link!\", link)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(link_time): \n",
    "    try:\n",
    "    #for link_time in links_times:\n",
    "        title,text = get_text(link_time[0])\n",
    "        time = link_time[1]\n",
    "        date = datetime.datetime.strptime(\"{} {} {} {}\".format(month, day, year, time), '%b %d %Y %I:%M%p')\n",
    "\n",
    "        #print(str(date)+'\\n'+title+'\\n'+text+'\\n'+link_time[0])\n",
    "        #print(str(date)+'\\n'+title+'\\n'+link_time[0])\n",
    "        mutex.acquire()\n",
    "        try:\n",
    "            add_news(str(date), title, text, link_time[0], ag)\n",
    "        finally:\n",
    "            mutex.release()\n",
    "    except HTTPError:\n",
    "        print(\"No page found by this link!\",r.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        for d in days:\n",
    "            try:\n",
    "                datetime.datetime(year=int(y),month=int(m),day=int(d))\n",
    "                r = requests.get(link.format(y, m, d))\n",
    "                r.raise_for_status() # check if page found\n",
    "                \n",
    "                soup = BS(r.content, 'html.parser')\n",
    "                span_list = soup.find_all('div' , attrs={'class':\"headlineMed\"})\n",
    "                header = str(soup.find_all('title')).split()\n",
    "                day, month, year = header[6], header[7], header[8]\n",
    "                \n",
    "                links_times = [[str(el).split('>')[1].split(\"=\")[1].replace('\"', ''), str(el).split()[-2]] for el in span_list \n",
    "                               if not str(el).split('>')[1].split(\"=\")[1].replace('\"', '').startswith('http://www.reuters.com/news/video')]\n",
    "            except HTTPError:\n",
    "                print(\"No page found by this link!\",r.status)\n",
    "                continue\n",
    "            except ValueError:\n",
    "                print(\"Day doesn't exist\")\n",
    "                continue\n",
    "            #numbers = [5, 10, 20, 1, 2, 3, 4, 7,7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
    "            pool = Pool(processes=10)\n",
    "            pool.map(get_info, links_times)\n",
    "                    \n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
