{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "from manager import *\n",
    "CPU = cpu_count() * 8\n",
    "\n",
    "years = ['2014',] #'2015']#, '2014', '2015', '2016', '2017']\n",
    "months = ['01', ]#'02', '03', '04', '05', '06']#, '07', '08', '09', '10', '11', '12']\n",
    "days = ['01','02']#, '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8186721801757812\n",
      "2 0\n",
      "0.6186215877532959\n",
      "2 0\n",
      "0.01031820774078369\n"
     ]
    }
   ],
   "source": [
    "def download_file(args):\n",
    "    url, out_dir, my_dict, y, d, m = args\n",
    "    r = requests.get(url[0])\n",
    "    html_doc = r.text\n",
    "    filename = os.path.join(out_dir, url[0][url[0].rfind(\"/\") + 1:] + \".txt\")\n",
    "    #filename = os.path.join(out_dir, url[0]+\".txt\")\n",
    "    soup = BS(html_doc, 'html.parser')\n",
    "    tag = soup.select_one(\".StandardArticleBody_body\")\n",
    "    #text = '\\n'.join([e.get_text() for e in soup.find_all('p')])\n",
    "    title = soup.html.title.string.strip().split(\" | \")[0]\n",
    "    not_found = soup.select_one(\"#sectionTitle\")\n",
    "    if tag != None:\n",
    "        date = str(datetime.datetime.strptime(\"{} {} {} {}\".format(m, d, y, url[1]), '%m %d %Y %I:%M%p'))\n",
    "        my_dict[url[0]] = [date, title, tag.text, \"Reuters\"]\n",
    "        #add_news(date, title, tag.text, url[0], \"Reuters\")\n",
    "    elif not_found:\n",
    "        print(url, not_found.text)\n",
    "    else:\n",
    "        print(url, \"Error\")\n",
    "    return my_dict\n",
    "\n",
    "def retrieve_links(date):\n",
    "    url = \"https://www.reuters.com/resources/archive/us/{}.html\".format(date)\n",
    "    r = requests.get(url)\n",
    "    html_doc = r.text\n",
    "    soup = BS(html_doc, 'html.parser')\n",
    "    span_list = soup.select(\".headlineMed\")\n",
    "    links_time = [[str(el).split('>')[1].split(\"=\")[1].replace('\"', ''), str(el).split()[-2]] for el in span_list]\n",
    "    urls2parse = []\n",
    "    for link in links_time:\n",
    "        if not link[0].startswith(\"http://www.reuters.com/news/video/\"):\n",
    "            urls2parse.append(link)\n",
    "    return urls2parse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    for y in years:\n",
    "        for m in months:\n",
    "            for d in days:\n",
    "                try:\n",
    "                    datetime.datetime(year=int(y),month=int(m),day=int(d))\n",
    "                except ValueError:\n",
    "                    print(\"No such date: {}/{}/{}\".format(y, m, d))\n",
    "                    continue\n",
    "                cpu_count = CPU\n",
    "                date = y+m+d\n",
    "                start = time.time()\n",
    "                urls2parse = retrieve_links(date)\n",
    "                #print(urls2parse)\n",
    "                out_dir = \"//home/ostapkharysh/Documents/bt_data/\" + date\n",
    "                #if not os.path.exists(out_dir):\n",
    "                #    os.makedirs(out_dir)\n",
    "                manager = Manager()\n",
    "                my_dict = manager.dict()\n",
    "                args = [(url, out_dir, my_dict, y, d, m) for url in urls2parse]\n",
    "                with Pool(cpu_count) as p:\n",
    "                    p.map(download_file, args)\n",
    "                for key in my_dict.keys():\n",
    "                    add_news(my_dict[key][0], my_dict[key][1], my_dict[key][2], key, my_dict[key][3])\n",
    "                # d = my_dict._getvalue()\n",
    "                print(time.time() - start)\n",
    "                print(len(d), len(urls2parse))\n",
    "    print((time.time() - start) /60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
